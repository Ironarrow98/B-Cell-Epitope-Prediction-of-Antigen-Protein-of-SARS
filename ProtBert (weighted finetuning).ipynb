{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ProtBert (weighted finetuning).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EW90OAS2O7Kh"},"source":["# TRANSFER LEARNING\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SOmF9HLaz5-_"},"source":["## Imbalanced Data"]},{"cell_type":"markdown","metadata":{"id":"Gf6JzmUB0Or9"},"source":["### Model Fine-tuning & Prediction"]},{"cell_type":"code","metadata":{"id":"4rk-4hRSz4zp"},"source":["import torch\n","from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n","from torch.utils.data import Dataset\n","import os\n","import pandas as pd\n","import requests\n","from tqdm.auto import tqdm\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import re\n","\n","model_name = 'Rostlab/prot_bert_bfd' # protbertbfd model\n","\n","class DeepLocDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, split=\"train\", tokenizer_name='Rostlab/prot_bert_bfd', max_length=64):\n","        # load data\n","        self.datasetFolderPath = 'Bcell_data/'\n","        self.trainFilePath = os.path.join(self.datasetFolderPath, 'input_train.csv')\n","        self.valFilePath = os.path.join(self.datasetFolderPath, 'input_val.csv')\n","        self.testFilePath = os.path.join(self.datasetFolderPath, 'input_test.csv')\n","\n","        # pretrained tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n","\n","        if split==\"train\":\n","          self.seqs, self.labels = self.load_dataset(self.trainFilePath)\n","        elif split==\"valid\":\n","          self.seqs, self.labels = self.load_dataset(self.valFilePath)\n","        else:\n","          self.seqs, self.labels = self.load_dataset(self.testFilePath)\n","\n","        self.max_length = max_length\n","\n","    def load_dataset(self,path):\n","        df = pd.read_csv(path,names=['peptide_seq','labels'],skiprows=1)\n","        self.labels_dic = {0:'Soluble', 1:'Membrane'}\n","        seq = list(df['peptide_seq'])\n","        label = list(df['labels'])\n","        assert len(seq) == len(label)\n","        return seq, label\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n","        seq = re.sub(r\"[UZOB]\", \"X\", seq) # replace unknown residues with X\n","\n","        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length) # truncate/pad sequences to same length\n","\n","        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n","        sample['labels'] = torch.tensor(self.labels[idx])\n","\n","        return sample\n","\n","train_dataset = DeepLocDataset(split=\"train\", tokenizer_name=model_name, max_length=64)\n","val_dataset = DeepLocDataset(split=\"valid\", tokenizer_name=model_name, max_length=64)\n","test_dataset = DeepLocDataset(split=\"test\", tokenizer_name=model_name, max_length=64)\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=4,   # batch size per device during training\n","    per_device_eval_batch_size=32,   # batch size for evaluation\n","    warmup_steps=1000,               # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=200,               # How often to print logs\n","    do_train=True,                   # Perform training\n","    do_eval=True,                    # Perform evaluation\n","    evaluation_strategy=\"epoch\",     # evalute after each epoch\n","    gradient_accumulation_steps=64,  # total number of steps before back propagation\n","    fp16=True,                       # Use mixed precision\n","    fp16_opt_level=\"02\",             # mixed precision mode\n","    run_name=\"ProBert-BFD-MS\",       # experiment name\n","    seed=3                           # Seed for experiment reproducibility 3x3\n",")\n","\n","trainer = Trainer(\n","    model_init=model_init,                # the pretrained Transformers model to be fine-tuned\n","    args=training_args,                   # training arguments, defined above\n","    train_dataset=train_dataset,          # training dataset\n","    eval_dataset=val_dataset,             # evaluation dataset\n","    compute_metrics = compute_metrics,    # evaluation metrics\n",")\n","\n","trainer.train()\n","trainer.save_model('models/')\n","\n","predictions, label_ids, metrics = trainer.predict(test_dataset)\n","print(predictions)\n","with open('predictions.txt', 'w') as f1:\n","    for i in predictions:\n","        f1.write(str(np.argmax(i, axis=0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRMOMbts0T6K"},"source":["### Evaluation Metrics"]},{"cell_type":"code","metadata":{"id":"20tys8fZ0CtM"},"source":["import numpy as np\n","\n","metrics = {'Precision':[], 'Recall': [], 'F1':[], 'Accuracy': []}\n","true_p, false_p, true_n, false_n = 0, 0, 0, 0\n","\n","with open('groundtruth.txt','r') as g: \n","  groundtruth = np.array(list(g.read()), dtype=int)\n","  \n","with open('predictions.txt','r') as p:\n","  predictions = np.array(list(p.read()), dtype=int)\n","\n","for i in range(len(groundtruth)):\n","  if groundtruth[i] == 0:\n","    if predictions[i] == 0:\n","      true_n += 1\n","    else:\n","      false_p += 1\n","  else:\n","    if predictions[i] == 0:\n","      false_n += 1\n","    else:\n","      true_p += 1\n","\n","#prec = true_p/(true_p + false_p)\n","#rec = true_p/(true_p + false_n)\n","#metrics['Precision'].append(prec)\n","#metrics['Recall'].append(rec)\n","#metrics['F1'].append(2*prec*rec / (prec + rec))\n","metrics['Accuracy'].append((true_p+true_n) / len(groundtruth))\n","print(true_p, false_p, true_n, false_n)\n","print(metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OOMFrDJT0uad"},"source":["Result: \n","\n","All predictions = 0\n","\n","Accuracy: 0.7307692307692307"]},{"cell_type":"markdown","metadata":{"id":"FY44XD8zz_J9"},"source":["## Weighted Classification"]},{"cell_type":"markdown","metadata":{"id":"2cCHASvv0XTT"},"source":["### Model Fine-tuning & Prediction"]},{"cell_type":"code","metadata":{"id":"X_z5PngnsjEb"},"source":["# pip -q install transformers seqeval\n","import torch\n","from torch import nn\n","from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n","from torch.utils.data import Dataset\n","import os\n","import pandas as pd\n","import requests\n","from tqdm.auto import tqdm\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import re\n","\n","model_name = 'Rostlab/prot_bert_bfd'\n","\n","class DeepLocDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, split=\"train\", tokenizer_name='Rostlab/prot_bert_bfd', max_length=64):\n","        self.datasetFolderPath = 'Bcell_data/'\n","        self.trainFilePath = os.path.join(self.datasetFolderPath, 'input_train.csv')\n","        self.valFilePath = os.path.join(self.datasetFolderPath, 'input_val.csv')\n","        self.testFilePath = os.path.join(self.datasetFolderPath, 'input_test.csv')\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n","\n","        if split==\"train\":\n","          self.seqs, self.labels = self.load_dataset(self.trainFilePath)\n","        elif split==\"valid\":\n","          self.seqs, self.labels = self.load_dataset(self.valFilePath)\n","        else:\n","          self.seqs, self.labels = self.load_dataset(self.testFilePath)\n","\n","        self.max_length = max_length\n","\n","    def load_dataset(self,path):\n","        df = pd.read_csv(path,names=['peptide_seq','labels'],skiprows=1)\n","        self.labels_dic = {0:'Soluble', 1:'Membrane'} # DO NOT CHANGE, WILL RESULT IN ALL PREDICTIONS = 0 (OR 1)\n","        seq = list(df['peptide_seq'])\n","        label = list(df['labels'])\n","        assert len(seq) == len(label)\n","        return seq, label\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n","        seq = re.sub(r\"[UZOB]\", \"X\", seq)\n","\n","        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n","\n","        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n","        sample['labels'] = torch.tensor(self.labels[idx])\n","\n","        return sample\n","\n","train_dataset = DeepLocDataset(split=\"train\", tokenizer_name=model_name, max_length=64)\n","val_dataset = DeepLocDataset(split=\"valid\", tokenizer_name=model_name, max_length=64)\n","test_dataset = DeepLocDataset(split=\"test\", tokenizer_name=model_name, max_length=64)\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","def model_init():\n","  return AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=4,   # batch size per device during training\n","    per_device_eval_batch_size=32,   # batch size for evaluation\n","    warmup_steps=1000,               # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=200,               # How often to print logs\n","    do_train=True,                   # Perform training\n","    do_eval=True,                    # Perform evaluation\n","    evaluation_strategy=\"epoch\",     # evalute after eachh epoch\n","    gradient_accumulation_steps=64,  # total number of steps before back propagation\n","    fp16=True,                       # Use mixed precision\n","    fp16_opt_level=\"02\",             # mixed precision mode\n","    run_name=\"ProBert-BFD-MS\",       # experiment name\n","    seed=3                           # Seed for experiment reproducibility 3x3\n",")\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","class_weights = torch.tensor([0.27, 1], dtype=torch.float, device=device)\n","\n","class WeightedTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = loss_fct(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","        \n","trainer = WeightedTrainer(\n","    model_init=model_init,                # the instantiated ?? Transformers model to be trained\n","    args=training_args,                   # training arguments, defined above\n","    train_dataset=train_dataset,          # training dataset\n","    eval_dataset=val_dataset,             # evaluation dataset\n","    compute_metrics = compute_metrics,    # evaluation metrics\n",")\n","\n","trainer.train()\n","trainer.save_model('models/')\n","\n","predictions, label_ids, metrics = trainer.predict(test_dataset)\n","print(predictions)\n","with open('predictions_w.txt', 'w') as f1:\n","    for i in predictions:\n","        f1.write(str(np.argmax(i, axis=0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lFHqmL00cKc"},"source":["### Evaluation Metrics"]},{"cell_type":"code","metadata":{"id":"646QCydiugNk"},"source":["import numpy as np\n","\n","metrics = {'Precision':[], 'Recall': [], 'F1':[], 'Accuracy': []}\n","true_p, false_p, true_n, false_n = 0, 0, 0, 0\n","\n","with open('groundtruth.txt','r') as g: \n","  groundtruth = np.array(list(g.read()), dtype=int)\n","  \n","with open('predictions_w.txt','r') as p:\n","  predictions = np.array(list(p.read()), dtype=int)\n","\n","for i in range(len(groundtruth)):\n","  if groundtruth[i] == 0:\n","    if predictions[i] == 0:\n","      true_n += 1\n","    else:\n","      false_p += 1\n","  else:\n","    if predictions[i] == 0:\n","      false_n += 1\n","    else:\n","      true_p += 1\n","\n","prec = true_p/(true_p + false_p)\n","rec = true_p/(true_p + false_n)\n","metrics['Precision'].append(prec)\n","metrics['Recall'].append(rec)\n","metrics['F1'].append(2*prec*rec / (prec + rec))\n","metrics['Accuracy'].append((true_p+true_n) / len(groundtruth))\n","print(true_p, false_p, true_n, false_n)\n","print(metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zg1UXMfT2Krc"},"source":["Results (max_length=64)\n","\n","Precision: 0.375609756097561\n","\n","Recall: 0.55\n","\n","F1: 0.4463768115942029\n","\n","Accuracy: 0.6326923076923077"]},{"cell_type":"markdown","metadata":{"id":"r8rihZiOVU_c"},"source":["Results (max_length=50): \n","\n","Precision: 0.39805825242718446\n","\n","Recall: 0.29285714285714287\n","\n","F1: 0.3374485596707819\n","\n","Accuracy: 0.6903846153846154"]},{"cell_type":"markdown","metadata":{"id":"rH4ZX9Eg1CFM"},"source":["EPOCH1\n","\n","'eval_loss': 0.6847864389419556, \n","\n","'eval_accuracy': 0.2710215427380125, \n","\n","'eval_f1': 0.42646254784034987, \n","\n","'eval_precision': 0.2710215427380125, \n","\n","'eval_recall': 1.0"]},{"cell_type":"markdown","metadata":{"id":"cFk986Bj0-1x"},"source":["EPOCH2\n","\n","'eval_loss': 0.6855406165122986, \n","\n","'eval_accuracy': 0.2710215427380125, \n","\n","'eval_f1': 0.42646254784034987, \n","\n","'eval_precision': 0.2710215427380125, \n","\n","'eval_recall': 1.0"]},{"cell_type":"markdown","metadata":{"id":"4tNjgRcC1T4T"},"source":["EPOCH3\n","\n","'eval_loss': 0.6859998106956482, \n","\n","'eval_accuracy': 0.2710215427380125, \n","\n","'eval_f1': 0.42646254784034987, \n","\n","'eval_precision': 0.2710215427380125, \n","\n","'eval_recall': 1.0"]}]}